---
layout: post
title: the indended yet unwanted C++ use cases
---

There is some unconscious goal, sort of a beauty all software developers try to achieve. I wonder where this ideal comes from. Have someone taught me or I read about? The more I think about the origins, the more I believe the goal one day just appeared to me. And once that happened it has been with me ever since. If I were to describe that beauty, I would say it is beyond things as trivial as proper indendation or code highlight. It operates rather on the philosophical plane. It is the "easiness of mind" you experience - the clarity of the concept expressed with code. But not entirely.

To develop software one requires enough knowledge to bootstrap confidence the problem tasked can eventually be solved. Whenever you face an uphill challenge you need some momentum to keep you going. Curiosity, wisdom or this elusive beauty. To those outside our little world what we do might seem like magic. None of our unique insight leaks outside. From our point of view the clarity roughly compares to what one might experience when following a mathematical proof. "Where that idea came from? Why this substitution?". I know what I describe partially escapes the familiar realm of science in favor of mysticism. You are welcome to feel excluded.

Yet the beauty is very real. In fact we rely on it to keep the wheels turning. Let's consider what happens to a project lacking the clarity; let's take a look at enterprise software. It resembles a huge, overgrown and patched monstrosity. People who develop that are not oblivious to the overwhelming uglyness. In fact it is them who produce loads of jokes and comic strips in desperate attempt to alleviate the guilt. The guilt that it is them who abandoned the ultimate goal for a common paycheck. However, a big company has a big budget and one must keep on living. So people get over the guilt and keep monster alive. A transfusion a week. A transplant from time to time. And the monster persists. Perpetual wheel of misery emerges. Since monster receives no love, it has none for its unfortunate keepers.

Closure demands a few words said on how to recognize the uglyness; a sneak peek into our world for newcomers. The very first thing that comes to you attention is how ridiculously unfriendly enterprise software is. Take for instance oracle database. It is equiped with supposedly useful graphical user interface, however, unless you are an expert in that specific database flavor, that GUI is utterly useless. I understand that production database requires numerous and sophisticated features. However, I find any product that cannot be used at all unless you master all its features thoroughly flawed. On the other hand you have compilers like gcc or clang. Although these are very elaborate and powerful programs, you are welcome to limit yourself to the most basic functionality. You specify list of source files, optionally name of executable, and you are good to go. It is tempting to prove your intellectual superiority by developing software designed to be sophisticated. Unfortunately, no user would appreciate unnecessarily steep learning curve. In you best interest is to mitigate the inclination. One day you might be the user yourself.

In fact you are user already. This very browser is not your creation. You can have displeasure of reading these passages thanks to significant effort put into entire internet web stack. Effort much greater than any singular person could exert in its life. You are the user. And what would a user do if the software they cannot modify "misbehave"? Restart and try again? Reinstall? Clear cache? There are many eligible tricks and you have for sure tried some. Users tend to accept buggy software as long as they are accustomed to the defects. You, a developer, are unfortunately on colision course. Your new software is by definition unfamiliar. If you attempt to mimic other existing user interface you are in risk of hitting [uncanny valley](https://en.wikipedia.org/wiki/Uncanny_valley). In enterprise this effect is much enhanced. You are allowed to develop only in one direction - forward. There is no room to prototype anything. Once you slip into gravity well of enterprise recognition you are stuck with never ending backward campability. Any refactor is potentially feature-breaking and thus illegal.

The bigger the company the less effective central management is. Authority has to be outsourced to departments. A mistake is bound to happen. A buggy, deficient piece of software might be ordered and processed. Later down the road another department might require similar functionlity. The management is more than happy to reuse the software. Would you tell them you prefer to replace faulty software? To pay again - twice? Would you risk your carrier to expose a trouble or bite teeth to plough the thin soil riddled with stones? Would you learn visual basic to "print" somehow to microsoft excel? What is the price for your integrity?

You must not think we still are in the 80s. Today information systems are expected to communicate between departments. The pesky decentralization poping out must be conquered with extreme prejudice! Individial units are not free to build their own understanding. The right interpretation must be forced upon them. This is opinion of those who fail to see how the same thing under the same name can differ across organization. Take for instance "price". In accounting it designates how much a unit of product was selled for. In shop it is the money to charge for a unit now. For economist it is a function of suply and demand. The price has not just diffent meaning - history record, value per unit, function - but is placed in diffent time - past, present, future. However, the centralized system has place only for one definition. Which would that be?

The monster is built from incompatible body parts. The longer it lives the more effort is put into chiseling the parts to somehow fit together. Ever incresing maintenance cost must be its verion of death. The death by decay, by entropy. All that maintenance produces lots of heat but little progress. I cannot help but wonder why we devote our lifetimes to make the monster live but we deny it the bliss of death. We really should accept that every singular enterprise is going to eventually perish.

##

From observing grinding cogs we can conclude no more then the immediate reason of failure. We need to see examples of cogs well greased. I am choosing those from the software collection that keeps me efficient. *Bash* deserves the first place. I had opportunity to work with vide variety of Integrated Development Environments. And every time I find myself gravitating towards good old xterm running bash. Each graphical user interface turns out to be just a fancy facade employing a few clever commands. Once I know the commands the appeal of the GUI is gone. I know I hold no prejudice. I simply express myself easier with a *for* loop or a *pipe*. It helps greatly I can save my commands into a script. There are few tools to automate GUI. No wonder, simply put they pale in comparison to ordinary shell.

The second would be the famous pair of cmake and make. Even though a bit dated today they are still really impressive. The benefit is twofold. Firstly, I can outsource all those mundaine bulding procedures. Secondly, cmake introduces the language I can effortlessly express relations between modules with. As a result, I am willing to divide the codebase into modules. There are books written on good practices you should employ. Each tip requires you to consciously alter your course. I do not have to. With the right tool the right direction is the natural choice.

To choose the right tools is far less trivial than you think. I have seen far too many billable hours wasted only because a developer was strugling to communicate with the tool at hand. And that was not just me. IT is plagued by stalled tasks. Most are stalled due to lack of proper equipment. In such an enviroment developer is afraid to replace the familiar tool. There might be better tool for the job but no one would risk offending vengeful spirits. Yep, superstitions come into play when you lack control over your own work. The fear is so great they eventually utterly depend on that one tool. The tool becomes their god. I have been told again and again how great this and that IDE is. And I can see again and again how clueless my colleagues are anytime they are required to perform something nontrivial. They boast how much better they are than me by using latest framework with streamlined deployment, yet, in face of any trouble they offer nothing but bewildered look. How can they live with themselves? They are pathetic.

In each of those cases I find a recurring theme. An IDE takes upon itself to compile, build and manage entire project. What I do with git, cmake and make is poorly attempted by IDE. Understandably so. There is no way to cover all the options of those powerful tools with a simple GUI. It is much better to do one job best than many poorly. Enterprise software seems to share the same flaw. Company requires the software to perform multiple, diverse jobs at once. That would not be natural for any human worker. Thus software developer must perform extensive mental gimnastics before they can explain the program to a machine. We are capable of understanding elaborate human-to-human relations but how those translate to machine-to-machine relations? Barely at best.

Our interpersonal relations play significant role in how we interpret the world. Words like manager or worker are prevalent in software nomenclature. You should be surprised by this fact. In computing things like a queue, register or reference are used. To name anything "manager" requires level of abstract thinking reaching way outside the domain of computing. Abstraction is recognized as a valor without objection. I think this is a mistake. It may be counterproductive to think in terms of pointers to raw memory. But to imagine that same pointer is a database manager is simply inacurate. In my opinion to abstract means to recognize repeating theme and explain a function availing the theme. Computing machines have nothing in common with human hierarchy. And yet we mingle both. The fact that computer systems reflect the structure of the owner company is widely known. This misunderstaning comes from self-important people who think they are so much more important than the infrastructure that they lose sight of how the work is in fact done.

##

It should be evident by now what features make software useful. Moreover, that usefulness is impervious to time. Tools like *vim*, *curl*, *bash*, *git* or *posix* are decades old. If you juxtapose those with any microsoft windows, you shall be confused why windows is outdated after barely few years from release. I believe any tool designed to solve well stated problem should retain its functionality years later. Afterall, software does not wear off. I do not ignore the fact that users might prefer next piece of software. Let's take a look at *vim* and *tex*. First is competent text editor. If you were to work with a server, you would be happy to use the old editor designed for nongraphical user interface. It does not mean there are no better alternatives for those who work on a workstation with a video adapter. *vim* is simply the best in its category. Second is professional typesetting system. The problem of placing proper types onto page was well defined by Donald Knuth in late 70s. If you were to print a book or an article, you would face the same chanllange Knuth did. Obviously, there have been additions made to this very popular system. For instance *latex* or *xelatex*. But still the problem formulated decades ago has now the same solution as in 70s. It is up to us to recognize and acknowledge previous achievements to build ontop of them.

We should carefully formulate the problem in the language of its domain. Create a solution with as few assumptions as possible. Only working in that spirit we might create the simple solution for the simple problem. When all distractions are skimmed, we can finnaly access the one thing that makes our solution work - the simple, functional idea. That is what we should appreciate in any software. That is what keeps us productive. That is what the software cost is spent on. That is what software developers strive to create. This is the ultimate goal.

At this point I expect strong oposition to my findings. Some might argue that my analysis is not valid for anything but trivial systems. In the real world any useful system must juggle multitude of tasks. Real problems are too sophisticated to be reduced to a set of a simple ideas. Those are arguments of people with short attention span. They are unable to record enough observations in their mind to notice when an observation repeats. If you observe sinus plot from up close, you might thing it is a part of a conic section. It takes change of perspective to understand that the sinus plot is not infinitely complex but rather cyclic. I agree it is nontrivial to make such observation in the real world. Yet, to cower, to choose false sense of security is unsustainable. You should take pride in a simple solution you created than attempt to build another giant on clay feet.

##

My goal here is to investigate how recent additions to *c++* improve my ability to create those simple, functional ideas. There is a schism in software developers between proponents and opposition to new standards (*c++14*, *c++17* and soon *c++20*). You may missed that but the trouble is actually massive. How else would you summarize the situation when *c++* commitee extends the usage of templates while major corporations like google discourage metaprogramming? It is unfathomable why *c++* is not only full of but receives new language constructs which should (according to the community) be avoided. Why are there features clearly intended to be useful yet are at the same time not wanted? It is time to go beyond the so called good practices and make informative opinion of our own.

What should be established first is the method on how to assess a programming language. Since such a language is dedicated only to express a computer program one should measure how algorithmic building blocks can be expressed. "To measure" might be an overstatement. I can easily think of two contradicting measures. Surprisingly, often performance is not a factor. And yet performance or speed is the measure most can think of. In practice the real cost is how many hours regular software developer requires to finish an application. Unexpected, right? I am sorry to dissolve your illusion but only insignificant fraction of developer work force is in a field demanding high performance (for instance forex trading).

Maybe it is all about the expression problem? It cannot. That problem relies too much on individial's opinion to produce a reliable measure. Regardless, that is the thing you can see discussed on most conferences. A revolutionary solution is always promised to help you with the mundane part of your job. The more code examples of those solutions I see the more I am convinced those are deliberately designed to prove the author's point. And that makes an example not only useless but Machiavellian. People are not in there for a truth but their own gain. Unfortunately, those hidden advertisements have influence on developers. They will choose what seems familiar. They will unknowingly push your project into a venue of unfit tools. You must remain ever vigilant. It is you who are in charge not the tools we happened to use.

Since in a workday of a regular software developer the use of algorithms is scarce at best algorithmic problems are not used in examples presented on any conference, in a book or internet tutorial. This creates an illusion that everyone can be software developer. Programming languages are in constant danger of obscuring important implementation details. In *java* it happened when people tried to hide transportation layer in Remote Method Invocation. As a result, RMI failed and is the thing of the past barely anyone remembers. I would like to reverse that trend. I refuse to think in terms of "managers" and "workers". My programs are built in terms of queues, parsers, state machines and transactions. Abstraction is to serve me - not the other way.

That is why I will implement parser of a finite grammar in my example. Quite a few of our everyday challenges can be expressed in terms of a parser and grammar. To abandon those useful concepts is the mistake I shall correct. I want to parse simple message consisting of unknown number of lines. Each line contains a pair of key and value. You would need such parser to make use of a http message.

```
Location: http://www.google.com/
Content-Type: text/html; charset=UTF-8
Date: Thu, 10 Oct 2019 11:01:01 GMT
Expires: Sat, 09 Nov 2019 11:01:01 GMT
Cache-Control: public, max-age=2592000
```

Do you remember what grammar is? You should have understanding of english grammar. Basic layout of a sentence could be *Subject Verb*. Subject and Verb are nonterminals. A nonterminal symbol is a symbol involved into ruleset but not part of grammar output. It is a placeholder that must be replaced with a terminal symbol. In our practice we can interpret nonterminal as a category while terminal is a word belonging to that category. For instance category Subject and instances Alice or Bob. Grammar does not define category instead connection between terminal and nonterminal is defined by ruleset. Example ruleset for our english grammar could be

```
Subject -> Alice
Subject -> Bob
Verb -> stands
Verb -> walks
```

These rules are trivial for the purpose of demonstration. Both input and output of a rule can involve a sequence of syumbols both terminal and nonterminal alike. There is two additional symbols we might find useful. The empty symbol `E` allows us to replace nonterminal with nothing - to delete a nonterminal. The start symbol `S` is the first input we apply rules to. Remember that the purpose is to generate a sentence. In our problem the text already exists. We need to apply grammar in such a way that the sentence generated is the same as the text. The benefit is that during the process we will store what was put in place of nonterminals. Thus we will understand the meaning behind unprocessed text. With this short introduction to finite grammars we can define our parser.

```
S -> Message
Message -> Line \n Message
Message -> E
Line -> Key:Value
Key -> alnum Key
Key -> E
Value -> alnum Value
Value -> E
```

Message must consist of a Line nonterminal ended with new line character or be empty. Line must consist of nonterminals Key, Value seperated by colon. Key and Value are defined recursively as a sequence of alphanumerics with length zero or more. I used `alnum` as a shortcut. To add a rule for each alphanumerics character is formal but too boring. The are three important observations about this grammar. First is that each rule contains only singular nonterminal in its input. Second is that the choice of rule is dictated by the text. In fact there is always only one rule we can apply. If text is nonempty then Message cannot be `E`. You append `alnum` to Key or Value until cursor in text points out to nonalphanumeric character or all characters from input are processed. Third is that Value cannot contain colon. Our grammar is not capable of processing a http message, however, for our use that simplification is acceptable.

Now how do we use a grammar in text interpretation? You know how to apply rules. You know the purpose is to find the sequence of rules that produce the text. But how to explain that to a machine? If you tried to apply grammar to example text once or twice, you would notice that we store the combined output of all used rules in our mind but always focus on the first symbol. If it is terminal, then we compare with what our cursor points to. If those differ, our grammar failed to generate the text. Otherwise, we move cursor forward and remove the symbol from memory. Since we always deal with the first symbol our memory is organized into a stack. The operations we use are to remove one symbol from or add a few symbols to the stack.
